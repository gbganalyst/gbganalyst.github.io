[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I'm Ezekiel.",
    "section": "",
    "text": "I am a data science professional with over eight years of experience in statistical modeling, data analysis, and advanced analytics. I graduated with first-class honors in Statistics from the University of Ilorin, Nigeria, and hold a Master’s degree in Mathematical Sciences from the African Institute for Mathematical Sciences (AIMS), Rwanda, supported by a Mastercard scholarship.\n            \n            \nI am honored to be a Mastercard Scholar, Federal Government of Nigeria (FSB) Scholar, UNLEASH Talent, Heidelberg Laureate Forum (HLF) Fellow, and Capacity Accelerator Network (CAN) Fellow of the Global Partnership for Sustainable Development Data.\n            \n            \n            My professional journey includes pivotal roles at the Rwanda Revenue Authority (RRA) as a Consultant Data Scientist, where I contributed to developing a dynamic taxpayers’ data portal, and at Data Science Nigeria (DSN), where I led data analytics projects, trained companies in data analytics, and organized major AI hackathons. As a former Senior Data Analytics Manager at 54gene, I implemented advanced data management and quality control procedures. As a CAN Fellow at eHealth Africa, I enhanced public health surveillance for cholera outbreaks in Yobe State, Nigeria, utilizing geospatial machine learning and data-driven approaches.\n            \n            \n            \n  I am a passionate contributor to the data science community, having developed the bulkreadr and \n  forstringr  CRAN R programming packages and contributed to the Big Book of R and the 'ralger' package. I co-edited the \"Beginners’ Artificial Intelligence & Python Programming Book\" and helped create the Nigeria COVID-19 data repository for researchers. Moreover, I am part of the organizing team for IndabaX Nigeria, one of the largest AI conferences in the country.   \n      \n           \n           \n    As an active member of the Africa R programming community and a certified RStudio Tidyverse instructor, I have published numerous research papers and provided consultancy to clients globally. I excel in data wrangling, analysis, tool-building, and process automation, with a strong commitment to utilizing data-driven solutions to address real-world challenges and promote sustainable development."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Hi, I'm Ezekiel.",
    "section": "News",
    "text": "News\n\nMay 25, 2024: The bulkreadr package is now available on CRAN. bulkreadr is an R package that simplifies and streamlines the process of reading and processing large volumes of data. It allows users to efficiently read multiple sheets from Microsoft Excel/Google Sheets workbooks and multiple CSV files from a directory. Learn more.  bulkreadr Concept Map\nNovember 7, 2023: I had the privilege of participating as a Trust CAN Fellow at the Festival de Datos, sponsored by the United Nations, held in Punta del Este, Uruguay, from November 7 to 9, 2023.\nOctober 15, 2023: I am honored to have been selected by the Global Partnership for Sustainable Development Data for the Capacity Accelerator Network (CAN) Fellowship. My project focuses on identifying environmental factors driving cholera outbreaks in Yobe State to enhance public health surveillance and response efforts.\n\nSeptember 24, 2023: I attended the 10th Heidelberg Laureate Forum (HLF) in Germany as a young researcher. It was an incredible experience engaging with laureates and fellow researchers. Learn more.\nAugust 14, 2023: I facilitated a session on reproducible research using version control (GitHub/GitLab) at the 2023 Data Science Nigeria Artificial Intelligence Bootcamp. Learn more.\nOctober 12, 2023: The forstringr package is now available on CRAN. The goal of forstringr is to enable complex string manipulation in R, particularly for those familiar with LEFT(), RIGHT(), and MID() functions in Microsoft Excel. The package combines the power of stringr with other manipulation packages such as dplyr and tidyr. Learn more.\nSeptember 30, 2023: My article on fetching data from an API using R and Python is now complete. This tutorial will teach you how to fetch data from an external source using HTTP requests and parse it into a usable format. It is written in Quarto, a powerful tool for publishing reproducible research. Read it here.\nNovember 28, 2022: I was selected by UNESCO and The Abdus Salam International Centre for Theoretical Physics (ICTP) to attend the 9th Workshop on Collaborative Scientific Software Development and Management of Open-Source Scientific Packages at ICTP, Trieste, Italy, from November 28 to December 9, 2022. Read the report.\nNovember 10, 2021: I am excited to announce that I will be joining 54gene as a data specialist, where I will contribute to genomic data analysis aimed at equalizing precision medicine in Africa.\nOctober 21, 2020: Our data article “An Exploratory Assessment of a Multidimensional Healthcare and Economic Data on COVID-19 in Nigeria” has been published in the Data in Brief. A great collaboration among ten researchers from Data Science Nigeria, Olabisi Onabanjo University) and Federal University Lokoja. Here’s a link to the paper.\nOctober 22, 2020: I became the first RStudio Tidyverse Certified Instructor from Nigeria on October 22, 2020.\nMay 27, 2020: I received a grant to attend the European R users meeting (eRum) in Milan, Italy, from May 27 to May 30, 2020.\nMarch 1, 2020: I was honored to be invited as a guest at the Next Einstein Forum (NEF) Global Gathering 2020, which took place in Nairobi, Kenya, from December 8-10, 2020.\nDecember 4, 2019: I joined Data Science Nigeria as a Data Scientist.\nMay 30, 2018: I was selected from a pool of more than 7,000 applicants to attend the UNLEASH Global Innovation Lab in Singapore from May 30 to June 6, 2018."
  },
  {
    "objectID": "index.html#featured-blog-posts",
    "href": "index.html#featured-blog-posts",
    "title": "Hi, I'm Ezekiel.",
    "section": "Featured Blog Posts",
    "text": "Featured Blog Posts\n\n\n\n\n\n\n\n\n\n\nHow to fetch API data in R and Python\n\n\n\nAug 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Your Website with Quarto: Complete Tutorial and Template\n\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html",
    "href": "blog/website-tutorial-quarto/index.html",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "",
    "text": "Quick Links:     Download the free Template     Live preview of the template"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#my-template-for-you",
    "href": "blog/website-tutorial-quarto/index.html#my-template-for-you",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.1 My template for you",
    "text": "2.1 My template for you\nYou do not have to start the project of creating your own personal website from scratch because I have crafted a template for you. You can take the template, fill in your information, and leave it as-is. Alternatively, if you find that crafting your own website sparks joy for you, and you want to further delve into the nuts and bolts, you can simply take this template as a basis and extend your website from there.\nYou can take a look at the live preview here. You find the free direct download link for the entire template here.\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n \n\n\n \n\n\n\nThe first step is to download the website template. You can either clone the GitHub repository or download a .zip archive with the template here:\n\nDownload the template"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#opening-the-template-in-rstudio",
    "href": "blog/website-tutorial-quarto/index.html#opening-the-template-in-rstudio",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.2 Opening the template in RStudio",
    "text": "2.2 Opening the template in RStudio\nUnpack the archive and open the quarto-website-template.Rproj file in RStudio. If R and RStudio are not installed on your machine, check out this link. You should see a new Build tab in the upper-right panel of RStudio with a button Render Website:\n\n\n\nIf you open the template project, Rstudio should show a Build tab containing a Render Website button\n\n\nIf you do not see that tab and button, please use the most powerful debugging tool known to mankind: Restart RStudio."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#global-configuration",
    "href": "blog/website-tutorial-quarto/index.html#global-configuration",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.3 Global configuration",
    "text": "2.3 Global configuration\nOpen the file _quarto.yml in the root directory. This is where you control the settings of your website. The YAML format is a compromise between a machine-readable and a human-readable file. The good news is: You don’t have to write new YAML blocks; you just have to enter some info in the appropriate places. Let’s go through the _quarto.yml file step by step.\n\n2.3.1 Author info\nIt’s time to tell the readers about yourself. Fill in your details, such as your name (title field), a brief description, and a global image of yourself.\n\n\n2.3.2 Navigation\nThe navigation in the template is split in two parts. The left side contains icons for your social links, such as Twitter, GitHub, or email.\nThe right side links to other pages of your website. Each navigation entry consists of a title (text), followed by a link (href). Take a look at the example entries and continue the pattern as you see fit.\n\n\n2.3.3 Advanced settings\nThe rest of the _quarto.yml file just controls that we will indeed create a website, our preview tab doesn’t change all the time, and we can publish our website without complications. Don’t mess with this part unless you know what you are doing. If you do know what you are doing: Move fast and break things 😉\n\n\n2.3.4 More config tweaks\nThis blog post covers the fundamentals to set you up with a working website. Are you hungry for more and want to pull an all-nighter because creating a website can be super addictive? The official Quarto Documentation contains a bunch of other settings you can tweak via the _quarto.yml configuration. Happy hacking, and let me know if you find something you want me to cover in a future blog post!"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#the-landing-page",
    "href": "blog/website-tutorial-quarto/index.html#the-landing-page",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.4 The landing page",
    "text": "2.4 The landing page\nIn the root directory of the template folder, you find the file index.qmd. This is the landing page of your website. This is the first impression that visitors have from your website. It is a great opportunity to tell them a bit about yourself and spark their interest.\nIn this template, we are not designing a full landing page from scratch. Instead, we use one of the great template designs that are shipped with Quarto. Template inception. We will basically feed Quarto the info to create a pretty “About” section, where users see your image, some social links, and a brief description.\nOpen the /index.qmd file and inspect the header, that is, the part at the top of the source code which is enclosed in --- delimiters:\n\n\n/index.qmd\n\n---\nabout:\n  template: jolla\n  id: about-block\n  image: img/my_image.png\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/MarvinSchmittML\n    - icon: github\n      text: Github\n      href: https://github.com/marvinschmitt\n    - icon: linkedin\n      text: LinkedIn\n      href: https://www.linkedin.com/in/marvin-schmitt-a85b321a2/\n    - icon: envelope\n      text: Email\n      href: \"mailto:mail.marvinschmitt@gmail.com\"  \n---\n\nAll following fields are nested in the about field, which is our way of telling Quarto that all the subsequent information should be associated with our “About” section. The template is called jolla, and you can find other options here. The id field assigns a name to the “About” section such that you can insert it anywhere in your page. As the name might suggest, the image field contains the path of an image to display on your landing page. Finally, the links field contains a bunch of social links. Each social entry consists of three fields:\n\nicon: identifier of an icon, such as twitter or envelope. You can take a look at the Standard Bootstrap5 Icons for more icon names.\ntext: description of the entry, usually for accessibility\nhref: link, make sure to add mailto: before an email address\n\nAs you can see in the /index.qmd file, the “About” block is inserted in the actual document via\n\n\n/index.qmd\n\n::: {#about-block}\n:::\n\nand you can add a nice introduction about yourself below the block."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#customizing-the-colors",
    "href": "blog/website-tutorial-quarto/index.html#customizing-the-colors",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.5 Customizing the colors",
    "text": "2.5 Customizing the colors\nI have prepared a style file at html/styles.scss, which contains the core configuration from the fabulous website of Andrew Heiss. This section defines the colors of the website which are used throughout many pages.\n\n\nhtml/styles.scss\n\n$primary:   $teal!default;\n$secondary: $gray-700 !default;\n$success:   $green !default;\n$info:      $cyan !default;\n$warning:   $orange !default;\n$danger:    $red !default;\n$light:     $gray-400 !default;\n$dark:      $black !default;\n\nThe most important field is arguably $primary. If you want your website to look mainly green, just change that line to $primary: $green!default;. Note that $teal and $green are SCSS variables, which are defined further up in the file. If you want to fine-tune your color palette, you can change these HEX codes."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#adding-new-pages",
    "href": "blog/website-tutorial-quarto/index.html#adding-new-pages",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "2.6 Adding new pages",
    "text": "2.6 Adding new pages\nIf you want to add a new page with the name mypage to your website, create a folder mypage/ and add a file index.qmd in that folder. This has one subtle advantage over creating mypage.qmd in the base directory. If you created /mypage.qmd, the URL would be www.yourname.com/mypage.html. That exposed .html in the URL looks very 2010. Instead, you should go with /mypage/index.qmd and get the modern-looking URL www.yourname.com/mypage.\n\n\n\nAvoid .html in your URL by using a subfolder with index.html.\n\n\nOnce you have created your brand-new index.qmd, just copy the code skeleton of any other index.qmd from the template and start adding your content. If you wish to, add the newly created page to the navigation (see Section 2.3.2). That’s it, you have successfully extended your website 🚀"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#setting-up-your-github-account",
    "href": "blog/website-tutorial-quarto/index.html#setting-up-your-github-account",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.1 Setting up your GitHub account",
    "text": "3.1 Setting up your GitHub account\nFirst, you create a free GitHub account at www.github.com. This is the go-to place for many programmers from various fields for all things around developing software. GitHub is an invaluable resource if you want to collaborate with others or simply build a portfolio of your own work to strengthen your next job application. Even the comments section of this blog is powered by GitHub (as of March 2023).\nEven if you come to the conclusion that you don’t actually want to build a personal website: If you are writing code in any way, please create a GitHub account now if you don’t have one already 😉\nNote: If you deploy your website through GitHub pages, your website will live at &lt;username&gt;.github.io – so you might want to avoid usernames like N00bSlayer420. n00bslayer420.github.io might not look as cool on your resume, just saying."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#preparing-your-website-for-github-pages",
    "href": "blog/website-tutorial-quarto/index.html#preparing-your-website-for-github-pages",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.2 Preparing your website for GitHub pages",
    "text": "3.2 Preparing your website for GitHub pages\nThe template already contains all necessary settings for GitHub pages, so you don’t have to do anything in this section. However, it’s always good to have a superficial understanding of how things work, and it’s fairly staightforward in this case. We use the simplest form of GitHub pages: We build the entire website locally (i.e., in RStudio) and output the built page into the docs/ folder. This is configured in the output-dir field of the almighty _quarto.yml:\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\nWe commit this docs/ folder to GitHub, and GitHub pages simply uses that folder to serve our website. This has one important consequence: If we make any change to our website, we have to Render the website again in RStudio. That’s how the updates make it to the docs/ folder! Only changing the .qmd file is not enough because GitHub does not render our site directly from the .qmd source.\nIf you are not familiar with programming at all, just imagine it like this: You are a real estate agent with a downtown office. You have your current listings in your street-facing window. You always write your listings in Microsoft Word, and those documents are of course saved as .docx files on your computer. However, those documents somehow have to get from your computer to the window display.\nThat’s the job of your friendly assistant Paige. You and Paige have agreed on the following workflow. You have a folder docs/ where you save all current listings as PDF files. Paige will monitor this docs/ folder, and whenever there is a change, Paige will print the PDF and replace the listing in the window display. Sounds like a straightforward process, right? That’s pretty much what we are doing in our deployment pipeline as well:\n\n\n\n\nReal Estate Agent\nYou\n\n\n\n\nEnvironment\nMicrosoft Word\nRStudio\n\n\nSource files\n.docx\n.qmd\n\n\nOutput folder\ndocs/\ndocs/\n\n\nDeployment\nAssistant (Paige)\nGitHub Pages\n\n\nOutput format\n.pdf\n.html\n\n\nServing platform\nWindow display\n&lt;domain&gt;.github.io\n\n\n\nNote to advanced readers: You can set up a GitHub action to let GitHub build the website directly from the source files. But that deserves its own tutorial since some bugs persist which make things awkward (as of February 2023). If you would like to see automatic deployment covered in a future blog post, please let me know!\nThe second change to our website files is to include an empty file with the name .nojekyll (already included in the template). Usually, GitHub pages does some post-processing on your website files with Jekyll. Since Quarto handles all the processing we need, we want to deactivate Jekyll’s post-processing. Adding a .nojekyll file to your website root directory does exactly that: It bypasses post-processing via Jekyll."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#deployment",
    "href": "blog/website-tutorial-quarto/index.html#deployment",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.3 Deployment 🥳",
    "text": "3.3 Deployment 🥳\nWe are almost there!\nYou will now create a new repository with the name &lt;github_account_name&gt;.github.io, where &lt;github_account_name&gt; is the exact name of your GitHub account. It is important that you follow this pattern, because &lt;github_account_name&gt;.github.io is the only repository that will be hosted directly to the URL &lt;github_account_name&gt;.github.io.\nIn my case, my GitHub name is marvinschmitt, so my website repository would be called marvinschmitt.github.io and the URL of my website will be marvinschmitt.github.io.\nNavigate to the newly created repository, select Add files via Upload, and add the entire folder content of our RStudio website project. Your repository should be populated now and look somewhat similar to this:\n\n\n\nThis is what the repository should look like after you upload data.\n\n\nOn the GitHub repository, go to Settings and click on GitHub Pages. Make sure that GitHub pages will deploy from a branch (1), the branch is set to main or master (2), and GitHub Pages is served from the docs/ folder (3):\n\n\n\nGitHub pages deploys from a branch (1), the branch is set to main or master (2), and it serves from the docs/ folder\n\n\nGo back to the repository and you’ll see a brown dot indicating that a build is in progress. This can take a couple of minutes. Once the deploy is done, the indicator dot will turn green. That’s it. Congratulations, your website is now live at &lt;yourname&gt;.github.io! 🎈🥂🥳"
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#optional-managing-github-directly-through-git",
    "href": "blog/website-tutorial-quarto/index.html#optional-managing-github-directly-through-git",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "3.4 (Optional) Managing GitHub directly through git",
    "text": "3.4 (Optional) Managing GitHub directly through git\nManually uploading your files to GitHub after every update can feel tedious.\n\n3.4.1 Option 1: Connect GitHub and RStudio\nThis great tutorial shows how you can directly connect GitHub and RStudio with a few simple steps. This makes uploading updates to your website as easy as pushing “Commit” and “Push” in RStudio.\n\n\n3.4.2 Option 2: GitHub Desktop\nIf you don’t want to manage the GitHub repository directly in Rstudio but don’t want to use the command line either, you can use GitHub Desktop as an in-between solution. I can highly recommend GitHub Desktop, it’s really easy to set up and use, even if you don’t have any experience with git."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#general-web-development-tips",
    "href": "blog/website-tutorial-quarto/index.html#general-web-development-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.1 General web development tips",
    "text": "5.1 General web development tips\nHere are some general web development tips, ranging from file management and DNS over to HTML, CSS, and JavaScript.\n\n5.1.1 Relative links\n\nThanks to Paul Buerkner for requesting this\n\nSuppose you are editing the file projects/index.qmd and want to add a link to the page &lt;name&gt;.github.io/photography. You can of course link to the full URL &lt;name&gt;.github.io/photography, but that link will break if you change your domain name to www.&lt;name&gt;.com. Instead, you can use relative links. You only need to know that ../ takes you one layer back on the folder tree.\nExample: Starting from projects/index.qmd, you can use ../photography/index.qmd to walk back one layer into the root folder of your website, and then move into the folder photography/, where index.qmd of photography lives. This relative path is valid no matter what domain you use. It only breaks if you decide to remodel the entire structure of your website. But in this case, I assume you know what you are doing and you’re well off on your own 😉\n\n\n5.1.2 CSS flavors\n\nThanks to Andrew Heiss for making me aware of SCSS from studying the source code of his website\n\nQuarto supports some cool CSS variants, such as SCSS. In SCSS, you can add variables such as $orange to define your favorite color once and use it in many different places. This deserves a separate blog post, though.\n\n\n5.1.3 Include Order\nIf you mix HTML with CSS and JavaScript, make sure to include the files in the correct order. If your JavaScript manipulates existing HTML objects through the DOM, make sure to include the JavaScript file after the HTML body. If you include the JavaScript before the HTML body, JavaScript will not know that the HTML objects exist, and you will have a bad time. Been there, spent well over 2 hours debugging. 0/10, cannot recommend."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#quarto-specific-tips",
    "href": "blog/website-tutorial-quarto/index.html#quarto-specific-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.2 Quarto specific tips",
    "text": "5.2 Quarto specific tips\nHere are some tips that are specific to Quarto, Markdown, or RMarkdown.\n\n5.2.1 Adding a table of contents\n\nThanks to Paul Buerkner for requesting this\n\nThanks to Quarto, it’s super easy to add a table of contents. Just add the following lines to the header of your .qmd file, without any indentation:\n\n\nindex.qmd\n\ntoc: true\ntoc-title: Contents\ntoc-location: left\n\n\n\n5.2.2 Labels and references\nIf you are familiar with \\(\\LaTeX\\), you might appreciate proper references with \\label{sec:introduction} and \\autoref{sec:introduction}. The Quarto equivalent is {#sec-introduction} for the label and @sec-introduction for the reference."
  },
  {
    "objectID": "blog/website-tutorial-quarto/index.html#more-tips",
    "href": "blog/website-tutorial-quarto/index.html#more-tips",
    "title": "Create Your Website with Quarto: Complete Tutorial and Template",
    "section": "5.3 More tips?",
    "text": "5.3 More tips?\nIf you know any other quick tips that you would like to see here, just leave a comment below or drop me a message. I will be happy to include it (and credit you, of course) 🚀"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "GBG Analytics Blog",
    "section": "",
    "text": "Welcome to my blog! I am pleased to have you here. This platform is dedicated to a wide array of topics, providing insightful and engaging articles aimed at informing and inspiring. I invite you to explore the content, and I look forward to hearing your feedback. I trust that you will find the information both valuable and thought-provoking."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "GBG Analytics Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            August 19, 2024\n        \n        \n            Export Multiple Data Frames to Different Excel Worksheets in R\n            \n            \n                \n                \n                    R\n                \n                \n                \n                    Excel\n                \n                \n            \n            \n            In this tutoral you will learn how to export multiple dataframes in R to an Excel workbook, with each Excel worksheet corresponding to a distint dataframe.\n\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 14, 2024\n        \n        \n            How to fetch API data in R and Python\n            \n            \n                \n                \n                    API\n                \n                \n                \n                    R\n                \n                \n                \n                    Python\n                \n                \n            \n            \n            This tutorial will teach you how to fetch data from an external source using HTTP requests and parse the data into a usable format.\n\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            August 6, 2024\n        \n        \n            Data Analysis and Visualization\n            \n            \n                \n                \n                    Data Analysis\n                \n                \n                \n                    R\n                \n                \n                \n                    Visualisation\n                \n                \n            \n            \n            This R project analyzes a modified autism dataset of children's attributes, emphasizing data preprocessing, univariate statistics, and visualizations. Structured as a Q&A tutorial, it guides users through data cleaning, descriptive statistics, visualizations, and hypothesis testing to explore and interpret the dataset, addressing key questions about autism diagnosis and associated factors.\n\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "GBG Analytics Blog",
    "section": "2023",
    "text": "2023\n\n\n    \n    \n                  \n            June 21, 2023\n        \n        \n            Every Scientist Should Have a Website\n            \n            \n                \n                \n                    Website\n                \n                \n                \n                    Academia\n                \n                \n            \n            \n            How putting yourself out there helps you show your research, expand your network, control your own content, reach an audience, and enhance your opportunities.\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            March 28, 2023\n        \n        \n            Create Your Website with Quarto: Complete Tutorial and Template\n            \n            \n                \n                \n                    Website\n                \n                \n                \n                    Academia\n                \n                \n                \n                    Quarto\n                \n                \n            \n            \n            This tutorial and template will help you build your own personal website with Quarto. It is a full guide with step-by-step instructions for your personal or academic homepage.\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-2",
    "href": "blog/index.html#section-2",
    "title": "GBG Analytics Blog",
    "section": "2022",
    "text": "2022\n\n\n    \n    \n                  \n            November 10, 2022\n        \n        \n            Data Consolidation: Automatically Merge Excel Sheets with Python\n            \n            \n                \n                \n                    Python\n                \n                \n            \n            \n            Learn how to dynamically consolidate data from multiple Excel sheets into a single pandas DataFrame using Python. This tutorial covers automating sheet retrieval, merging data efficiently, and tracking source sheets—all while simplifying your data processing workflow. Perfect for streamlining recurring reports and handling complex datasets!\n\n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            October 20, 2022\n        \n        \n            Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python\n            \n            \n                \n                \n                    Python\n                \n                \n            \n            \n            In this quick tutorial, you'll learn how to easily merge several CSV files into one using Python. Whether you're working with a few small files or large datasets, this guide shows you how to use Pandas for simple tasks or `Dask` when your data is too big to handle. It's a practical, straightforward approach for anyone looking to combine CSV files efficiently.\n\n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/data-consolidation-excel/index.html#step-1-loading-the-excel-file-and-retrieving-sheet-names",
    "href": "blog/data-consolidation-excel/index.html#step-1-loading-the-excel-file-and-retrieving-sheet-names",
    "title": "Data Consolidation: Automatically Merge Excel Sheets with Python",
    "section": "Step 1: Loading the Excel File and Retrieving Sheet Names",
    "text": "Step 1: Loading the Excel File and Retrieving Sheet Names\nThe first task is to load the Excel file and retrieve all available sheet names. We use the pd.ExcelFile() function to load the file, and the sheet_names attribute to dynamically retrieve the list of sheets.\n\n# Load the Excel file\nexcel_file = pd.ExcelFile('Diamonds.xlsx')\n\n# Retrieve all sheet names\nsheet_names = excel_file.sheet_names\n\nAt this point, sheet_names will be a list of all the sheet names in the Excel file. For example,\n\nsheet_names\n\n['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']"
  },
  {
    "objectID": "blog/data-consolidation-excel/index.html#step-2-reading-and-concatenating-data-from-all-sheets",
    "href": "blog/data-consolidation-excel/index.html#step-2-reading-and-concatenating-data-from-all-sheets",
    "title": "Data Consolidation: Automatically Merge Excel Sheets with Python",
    "section": "Step 2: Reading and Concatenating Data from All Sheets",
    "text": "Step 2: Reading and Concatenating Data from All Sheets\nWith the sheet names available, the next step is to read the data from each sheet and concatenate it into a single DataFrame. We use a dictionary comprehension to load each sheet into a DataFrame, and then use the concat() function to combine them. We also add a column that records the name of the sheet from which each row originated.\n\n# Read all sheets and concatenate them into one DataFrame, adding a sheet_name column\ndf_dict = {sheet: pd.read_excel(excel_file, sheet_name=sheet) for sheet in sheet_names}\n\ndata = pd.concat(df.assign(sheet_name=name) for name, df in df_dict.items())\n\nThis code reads each sheet into a dictionary (df_dict), where the keys are sheet names and the values are DataFrames. The concat() function then merges all the DataFrames together into a single DataFrame, and the sheet_name column ensures that each row has a record of its original sheet."
  },
  {
    "objectID": "blog/data-consolidation-excel/index.html#step-3-inspecting-the-combined-data",
    "href": "blog/data-consolidation-excel/index.html#step-3-inspecting-the-combined-data",
    "title": "Data Consolidation: Automatically Merge Excel Sheets with Python",
    "section": "Step 3: Inspecting the Combined Data",
    "text": "Step 3: Inspecting the Combined Data\nOnce the data has been successfully consolidated, it’s always a good practice to inspect it and verify that everything was combined correctly. The head() and tail() functions are useful for this.\n\n# View the first few rows of the data\ndata.head()\n\n\n\n\n\n\n\n\ncarat\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\nsheet_name\n\n\n\n\n0\n2.00\nI\nSI1\n65.9\n60.0\n13764\n7.80\n7.73\n5.12\nFair\n\n\n1\n0.70\nH\nSI1\n65.2\n58.0\n2048\n5.49\n5.55\n3.60\nFair\n\n\n2\n1.51\nE\nSI1\n58.4\n70.0\n11102\n7.55\n7.39\n4.36\nFair\n\n\n3\n0.70\nD\nSI2\n65.5\n57.0\n1806\n5.56\n5.43\n3.60\nFair\n\n\n4\n0.35\nF\nVVS1\n54.6\n59.0\n1011\n4.85\n4.79\n2.63\nFair\n\n\n\n\n\n\n\n\n# View the last few rows of the data\ndata.tail()\n\n\n\n\n\n\n\n\ncarat\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\nsheet_name\n\n\n\n\n55\n0.41\nE\nVS1\n62.3\n57.0\n1153\n4.77\n4.74\n2.96\nIdeal\n\n\n56\n0.40\nE\nVS1\n62.1\n55.0\n898\n4.75\n4.79\n2.96\nIdeal\n\n\n57\n0.53\nG\nSI1\n60.7\n56.0\n1371\n5.22\n5.27\n3.18\nIdeal\n\n\n58\n0.45\nI\nVS1\n62.1\n55.0\n825\n4.90\n4.92\n3.05\nIdeal\n\n\n59\n1.03\nG\nSI1\n61.6\n55.0\n5518\n6.48\n6.52\n4.00\nIdeal\n\n\n\n\n\n\n\nThis allows you to verify that:\n\nData from all sheets has been loaded.\nThe sheet_name column contains the correct sheet names for each row."
  },
  {
    "objectID": "blog/data-consolidation-excel/index.html#step-4-saving-the-consolidated-data",
    "href": "blog/data-consolidation-excel/index.html#step-4-saving-the-consolidated-data",
    "title": "Data Consolidation: Automatically Merge Excel Sheets with Python",
    "section": "Step 4: Saving the Consolidated Data",
    "text": "Step 4: Saving the Consolidated Data\nIf you want to save the consolidated data for further use, you can export it to a new Excel file or a CSV file using the following code:\n\nSaving to an Excel file:\n\ndata.to_excel('Consolidated_Data.xlsx', index=False)\n\n\n\nSaving to a CSV file:\n\ndata.to_csv('Consolidated_Data.csv', index=False)"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#autism-data",
    "href": "blog/data-analysis-and-visualization/index.html#autism-data",
    "title": "Data Analysis and Visualization",
    "section": "Autism Data",
    "text": "Autism Data\nTo follow along with this tutorial, you can download the dataset below:"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#import-required-packages",
    "href": "blog/data-analysis-and-visualization/index.html#import-required-packages",
    "title": "Data Analysis and Visualization",
    "section": "Import Required Packages",
    "text": "Import Required Packages\n\n# Ensure your computer is connected to the internet!\n\npackages_needed &lt;- c(\n  \"tidyverse\", \"vtable\", \"inspectdf\", \"knitr\",\n  \"kableExtra\", \"gt\", \"patchwork\", \"treemap\"\n)\n\nif (!require(install.load)) {\n  install.packages(\"install.load\")\n}\n\ninstall.load::install_load(packages_needed)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#import-the-dataset",
    "href": "blog/data-analysis-and-visualization/index.html#import-the-dataset",
    "title": "Data Analysis and Visualization",
    "section": "Import the Dataset",
    "text": "Import the Dataset\n\nchild_data &lt;- read_csv(\"child.csv\")\n\nThe code below cleans the dataset by removing any whitespace and apostrophes from character variables.\n\nclean_data &lt;- child_data %&gt;%\n  mutate(across(where(is.character), ~ str_squish(str_remove_all(., pattern = \"'\")))) %&gt;%\n  mutate(across(5:11, as.factor))\n\nclean_data &lt;- clean_data %&gt;% mutate(\n  autism = fct_relevel(autism, \"YES\"),\n  autismFH = fct_relevel(autismFH, \"yes\")\n)"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution",
    "href": "blog/data-analysis-and-visualization/index.html#solution",
    "title": "Data Analysis and Visualization",
    "section": "Solution",
    "text": "Solution\n\ndat1 &lt;- clean_data %&gt;%\n  filter(residence %in% c(\"Australia\", \"Germany\", \"Italy\", \"India\")) %&gt;%\n  count(residence) %&gt;%\n  mutate(prop = n / sum(n))\n\ndat1 %&gt;%\n  gt() %&gt;%\n  tab_spanner(label = \"Statistics\", columns = vars(n, prop))\n\n\n\n\n\n\n\nresidence\nStatistics\n\n\nn\nprop\n\n\n\n\nAustralia\n23\n0.33823529\n\n\nGermany\n1\n0.01470588\n\n\nIndia\n42\n0.61764706\n\n\nItaly\n2\n0.02941176\n\n\n\n\n\n\ndat1 %&gt;%\n  ggplot(aes(x = reorder(residence, prop), y = prop, fill = residence)) +\n  geom_col(width = 0.5, show.legend = FALSE) +\n  theme_bw() +\n  labs(x = \"Residence\", y = \"Relative Proportion\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\nThe visualization indicates that most children in this subset reside in India. An alternative visualization could be a pie chart.\nAdvantages of a Pie Chart:\n\nSimple and easy to interpret\nVisually clear, especially with few categories\nIdeal for presenting proportions"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution-1",
    "href": "blog/data-analysis-and-visualization/index.html#solution-1",
    "title": "Data Analysis and Visualization",
    "section": "Solution",
    "text": "Solution\n\nSolution: Part 1Solution: Part 2\n\n\n\nplot_box &lt;- function(df, cols, col_x = \"autism\") {\n  for (col in cols) {\n    p &lt;- ggplot(df, aes(x = .data[[col_x]], y = .data[[col]], fill = .data[[col_x]])) +\n      geom_boxplot(\n        show.legend = FALSE, width = 0.2, outlier.size = 1,\n        outlier.shape = 5, outlier.colour = \"purple\"\n      ) +\n      scale_fill_manual(values = c(\"YES\" = \"red\", \"NO\" = \"green\")) +\n      labs(y = col, x = NULL, title = paste0(\"Boxplot of \", col, \" by autism status\")) +\n      theme(\n        axis.text.x = element_text(face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\")\n      )\n    print(p)\n  }\n}\n\nnum_cols &lt;- clean_data %&gt;%\n  select_if(is.numeric) %&gt;%\n  colnames()\n\nplot_box(clean_data, num_cols)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox plots help assess whether a feature is useful in distinguishing between children with and without autism. Here, the cost of testing is higher for children diagnosed with autism, and both test scores are higher in children with autism.\n\n\n\nplot_bars &lt;- function(df, cat_cols, facet_var) {\n  for (col in cat_cols) {\n    p &lt;- ggplot(df, aes(x = .data[[col]], fill = .data[[col]])) +\n      geom_bar(show.legend = FALSE, width = 0.3) +\n      labs(\n        x = col, y = \"Number of Children\",\n        title = str_c(\"Bar Plot of \", col),\n        subtitle = paste0(\"Faceted by Autism Status\")\n      ) +\n      facet_wrap(vars({{ facet_var }}), scales = \"free_y\") +\n      theme(\n        axis.title.y = element_text(size = 12, face = \"bold\"),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\")\n      )\n    print(p)\n  }\n}\n\ncat_cols &lt;- clean_data %&gt;%\n  select_if(is.factor) %&gt;%\n  colnames()\n\ncat_cols &lt;- cat_cols[-c(5, 7)] # Removing the class label\n\nplot_bars(clean_data, cat_cols, autism)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese plots show that features like relation, autismFH, jaundice, ethnicity, and gender have significant differences in their distribution based on autism status."
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#task-3a",
    "href": "blog/data-analysis-and-visualization/index.html#task-3a",
    "title": "Data Analysis and Visualization",
    "section": "Task 3a",
    "text": "Task 3a\nApply data analysis techniques in order to answer each of the questions below, justifying the steps you have followed and the limitations (if any) of your analysis. If a question cannot be answered explain why.\n\nIs the mean score different for children with autism compared to those without, using a significance level of 0.05?\nIs there a difference of at least 1 in mean scores between children with a family history of autism and those without?"
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution-3a",
    "href": "blog/data-analysis-and-visualization/index.html#solution-3a",
    "title": "Data Analysis and Visualization",
    "section": "Solution 3a",
    "text": "Solution 3a\nPart 1: Testing Variance Homogeneity\nOne of the assumptions of t-test of independence of means is homogeneity of variance (equal variance between groups).\nThe statistical hypotheses are:\n\nNull Hypothesis (\\(H_0\\)): The variances of the two groups are equal.\nAlternative Hypothesis (\\(H_a\\)): The variances are different.\n\n\ncar::leveneTest(score ~ autism, data = clean_data)\n\n\n  \n\n\n\nInterpretation: The p-value is less than 0.05, indicating a significant difference in variances.\n\nt.test(score ~ autism, data = clean_data, alternative = \"two.sided\", var.equal = FALSE)\n\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  score by autism\n#&gt; t = 24.242, df = 280.24, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: true difference in means between group YES and group NO is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  3.584006 4.217497\n#&gt; sample estimates:\n#&gt; mean in group YES  mean in group NO \n#&gt;          8.411348          4.510596\n\n\nThere is a significant difference in mean scores between children with autism (M = 8.\n41, SD = 1.19) and those without (M = 4.51, SD = 1.54); t(280.24) = 24.242, p &lt; 0.05.\nPart 2: Testing Family History\n\ncar::leveneTest(score ~ autismFH, data = clean_data)\n\n\n  \n\n\n\nInterpretation: The p-value is greater than 0.05, indicating no significant difference in variances.\n\nt.test(score ~ autismFH, data = clean_data, alternative = \"two.sided\", var.equal = TRUE)\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  score by autismFH\n#&gt; t = -1.3311, df = 290, p-value = 0.1842\n#&gt; alternative hypothesis: true difference in means between group yes and group no is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -1.2348058  0.2384339\n#&gt; sample estimates:\n#&gt; mean in group yes  mean in group no \n#&gt;          5.979592          6.477778\n\n\nThere is no significant difference in mean scores between children with a family history of autism (M = 5.98, SD = 2.60) and those without (M = 6.48, SD = 2.35); t(290) = -1.3311, p = 0.1842."
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#task-3b",
    "href": "blog/data-analysis-and-visualization/index.html#task-3b",
    "title": "Data Analysis and Visualization",
    "section": "Task 3b",
    "text": "Task 3b\n\nPredict the alternative score (score2) for a child with a standard score of 7.\nPredict the alternative score (score2) for a child with a standard score of 12."
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution-3b",
    "href": "blog/data-analysis-and-visualization/index.html#solution-3b",
    "title": "Data Analysis and Visualization",
    "section": "Solution 3b",
    "text": "Solution 3b\nPart 1:\n\nmodel &lt;- lm(score2 ~ score, data = clean_data)\n\npredict(model, data.frame(score = 7))\n\n#&gt;        1 \n#&gt; 7.007971\n\n\n\\(score2 = 7.01\\)\nThe predicted value of the alternative score (score2) for a child with a standard score of 7 is 7.01.\nPart 2:\n\npredict(model, data.frame(score = 12))\n\n#&gt;        1 \n#&gt; 11.98049\n\n\n\\(score2 = 11.98\\)\nThe predicted value of the alternative score (score2) for a child with a standard score of 12 is 11.98."
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution-2",
    "href": "blog/data-analysis-and-visualization/index.html#solution-2",
    "title": "Data Analysis and Visualization",
    "section": "Solution",
    "text": "Solution\n\nclean_data &lt;- clean_data %&gt;%\n  mutate(ageGroup = case_when(age &gt;= 6 ~ \"6 and over\", TRUE ~ \"Five and under\"))\n\nclean_data %&gt;%\n  ggplot(aes(x = cost, y = score, color = ageGroup)) +\n  geom_line() +\n  facet_grid(ageGroup ~ autismFH, scales = \"free\") +\n  labs(title = \"Cost vs. Score by Age Group and Family History of Autism\")\n\n\n\n\n\n\n\n\nChildren aged five years and under with a family history of autism tend to have lower costs associated with standard autism testing."
  },
  {
    "objectID": "blog/data-analysis-and-visualization/index.html#solution-3",
    "href": "blog/data-analysis-and-visualization/index.html#solution-3",
    "title": "Data Analysis and Visualization",
    "section": "Solution",
    "text": "Solution\n\nanother_dataset &lt;- read_csv(\"independent_dataset.csv\")\n\n\n# Function to calculate the size of confidence intervals\nconf.size &lt;- function(dataset, level = 0.90) {\n  t_test &lt;- t.test(dataset[, 2] %&gt;% pull(), conf.level = level)\n  print(t_test$conf.int)\n}\n\n\nconf.size(another_dataset, level = 0.9)\n\n#&gt; [1] 48.41712 50.42498\n#&gt; attr(,\"conf.level\")\n#&gt; [1] 0.9\n\n\nThe 90% confidence interval for the average percentage of positive cases of autism ranges from 48.42% to 50.42%.\n\nconf.size(another_dataset, level = 0.95)\n\n#&gt; [1] 48.20473 50.63738\n#&gt; attr(,\"conf.level\")\n#&gt; [1] 0.95\n\n\nThe 95% confidence interval for the average percentage of positive cases of autism ranges from 48.20% to 50.64%.\n\nconf.size(another_dataset, level = 0.98)\n\n#&gt; [1] 47.94336 50.89875\n#&gt; attr(,\"conf.level\")\n#&gt; [1] 0.98\n\n\nThe 98% confidence interval for the average percentage of positive cases of autism ranges from 47.94% to 50.90%.\n\nOverall Interpretation\n\nAs the confidence level increases, the confidence intervals become wider, making it harder to reject the null hypothesis.\n\n\n\n\nCongratulations!\n\n\n\nI hope you found this tutorial helpful. For more updates on data science with R, Python, and Excel, follow me on Twitter and LinkedIn. The GitHub repository for this tutorial is available here.\n\n\n\nCheers!"
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#what-is-an-api",
    "href": "blog/api-in-r-and-python/index.html#what-is-an-api",
    "title": "How to fetch API data in R and Python",
    "section": "What is an API?",
    "text": "What is an API?\n\n\n\nImage source: Unsplash\n\n\nAn API (Application Programming Interface) is a set of protocols, routines, and tools used for building software applications. It is essentially a set of rules and methods that data analyst or software developers can use to interact with and access the services and features provided by another application, service or platform.\nIn simpler terms, an API allows different software applications to communicate with each other and share data in a standardized way. With APIs, developers or analysts can get data without needing to scrape the website, manually download it, or directly go to the company from which they need it.\n\n\n\n\n\n\nFigure 1: How API work\n\n\n\nFor example, if you want to integrate a weather forecast feature into your app, you can use a weather API that provides the necessary data, rather than building the entire feature from scratch. This allows you to focus on the unique aspects of your app, without worrying about the underlying functionality."
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#type-of-request-methods",
    "href": "blog/api-in-r-and-python/index.html#type-of-request-methods",
    "title": "How to fetch API data in R and Python",
    "section": "Type of request methods",
    "text": "Type of request methods\n\n\n\n\n\n\n\n\nFigure 2: Types of request method\n\n\n\n\n\nAs shown in Figure 2, the most common type of API request method is GET."
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#general-rules-for-using-an-api",
    "href": "blog/api-in-r-and-python/index.html#general-rules-for-using-an-api",
    "title": "How to fetch API data in R and Python",
    "section": "General rules for using an API",
    "text": "General rules for using an API\nTo use an API to extract data, you will need to follow these steps:\n\nFind an API that provides the data you are interested in. This may involve doing some research online to find available APIs.\nFamiliarize yourself with the API’s documentation to understand how to make requests and what data is available.\nUse a programming language to write a script that sends a request to the API and receives the data. Depending on the API, you may need to include authentication parameters in the request to specify the data you want to receive.\nParse the data you receive from the API to extract the information you are interested in. The format of the data will depend on the API you are using, and may be in JSON, XML, or another format.\nProcess the extracted data in your script, or save it to a file or database for later analysis."
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#how-to-fetch-api-data-using-a-programming-language",
    "href": "blog/api-in-r-and-python/index.html#how-to-fetch-api-data-using-a-programming-language",
    "title": "How to fetch API data in R and Python",
    "section": "How to fetch API data using a programming language",
    "text": "How to fetch API data using a programming language\n\nwith R\nThere are several packages available in R for consuming APIs. Some of the most commonly used packages are:\n\nhttr: This package provides convenient functions for making HTTP requests and processing the responses.\njsonlite: This package is used for parsing JSON data, which is a common format for API responses.\nRCurl: This package is a wrapper around the libcurl library, which is a powerful and versatile HTTP client library.\n\nTo get data from an API in R, you need to follow these steps:\n\nInstall the required packages by running the following command in the R console:\n\n\ninstall.packages(c(\"httr\", \"jsonlite\", \"RCurl\"))\n\n\nLoad the packages by running the following command:\n\n\nlibrary(httr)\n\nlibrary(jsonlite)\n\nlibrary(RCurl)\n\n\nMake an API request by using the GET function from the httr package. The API endpoint should be passed as an argument to this function.\n\n\nresponse &lt;- GET(\"https://api.example.com/endpoint\")\n\n\nCheck the status code of the response to see if the request was successful. A status code of 200 indicates a successful request.\n\n\nstatus_code &lt;- status_code(response)\n\n\nExtract the data from the response. If the API returns data in JSON format, you can use the fromJSON function from the jsonlite package to parse the data. Store the data in a variable for later use.\n\n\napi_data &lt;- fromJSON(content(response, as = \"text\"))\n\nThese are the basic steps to get data from an API in R. Depending on the API, you may need to pass additional parameters or authentication information in your request. For example,\n\nresponse &lt;- GET(\n  \"https://api.example.com/endpoint\",\n  authenticate(\n    user = \"API_KEY_HERE\",\n    password = \"API_PASSWORD_HERE\",\n    type = \"basic\"\n  )\n)\n\n\n\nwith Python\nTo use an API in Python, you can use a library such as requests or urllib to send HTTP requests to the API and receive responses. Here’s an example of how to use an API in Python using the requests library:\n\nimport requests\n\n# Define the API endpoint URL and parameters\nendpoint = 'https://api.example.com/data'\n\nparams = {'param1': 'value1', 'param2': 'value2'}\n\n# Send a GET request to the API endpoint\nresponse = requests.get(endpoint, params = params)\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Parse the response JSON data\n    data = response.json()\n\n    # Process the data, for example by printing it to the console\n    print(data)\nelse:\n    print(f'Error: {response.status_code}')\n\nIn this example, we’re using the requests library to send a GET request to an API endpoint at https://api.example.com/data, passing two parameters (param1 and param2) in the request. The requests.get() method returns a Response object, which we can use to check the response status code and parse the response data.\nIf the status code is 200, we can assume the request was successful, and we can parse the response data using the response.json() method, which converts the JSON-formatted response to a Python object. We can then process the data as needed, for example by printing it to the console.\nOf course, the exact API endpoint and parameters will depend on the specific API that you are using, and you’ll need to consult the API documentation to learn how to construct your request correctly. But this example should give you a sense of the general process involved in using an API in Python."
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#practical-example-in-r-and-python",
    "href": "blog/api-in-r-and-python/index.html#practical-example-in-r-and-python",
    "title": "How to fetch API data in R and Python",
    "section": "Practical example in R and Python",
    "text": "Practical example in R and Python\nWe will use R and Python to fetch the API data without and with the key.\n\nWithout the key\nIn this example, we will use an API from a site called  that uses an API without an API key. In this case, we will be using a GET request to fetch the API data at the food banks using this link: https://www.givefood.org.uk/api/2/foodbanks. Please follow the steps in Section 4.1 for R and Section 4.2 for Python.\n\nRPython\n\n\n\nlibrary(httr)\n\nlibrary(jsonlite)\n\nlibrary(dplyr)\n\nresponse &lt;- GET(\"https://www.givefood.org.uk/api/2/foodbanks\")\n\nstatus_code(response)\n\n#&gt; [1] 200\n\nfood_dataframe &lt;- fromJSON(content(response, as = \"text\"), flatten = TRUE)\n\nfood_dataframe %&gt;%\n  dim()\n\n#&gt; [1] 919  27\n\nfood_dataframe %&gt;%\n  head()\n\n\n  \n\n\n\n\n\n\nimport requests\n\nimport pandas as pd\n\nresponse = requests.get(\"https://www.givefood.org.uk/api/2/foodbanks\")\n\n# Check if the request was successful\n\nprint(response.status_code)\n\n#&gt; 200\n\n# Parse the response JSON data\n\nfood_json = response.json()\n\n# Convert to a pandas dataframe\n\nfood_dataframe = pd.json_normalize(food_json)\n\nfood_dataframe.shape\n\n#&gt; (919, 27)\n\nfood_dataframe.head()\n\n#&gt;                     name  ...                                 politics.urls.html\n#&gt; 0    Sherborne Food Bank  ...  https://www.givefood.org.uk/needs/in/constitue...\n#&gt; 1       Himmah Food Bank  ...  https://www.givefood.org.uk/needs/in/constitue...\n#&gt; 2        Felix Multibank  ...  https://www.givefood.org.uk/needs/in/constitue...\n#&gt; 3    St Thomas Food Bank  ...  https://www.givefood.org.uk/needs/in/constitue...\n#&gt; 4  North Lochs Food Bank  ...  https://www.givefood.org.uk/needs/in/constitue...\n#&gt; \n#&gt; [5 rows x 27 columns]\n\n\nIn this example, we use the pd.json_normalize() method to flatten the list of dictionaries and create a dataframe from it. The resulting dataframe has columns for each key in the JSON objects.\n\n\n\n\n\nWith the key\nIn this example, we will use an API from  that uses an API key. In this case, we will use a GET request to fetch data for analyst jobs based in London from the Jobseeker API. Please follow the steps in Section 4.1 for R and Section 4.2 for Python, and sign up for the API key at the Jobseeker website.\n\nRPython\n\n\n\nlibrary(httr)\n\nlibrary(jsonlite)\n\nlibrary(dplyr)\n\n# Create a GET response to call the API\n\nresponse &lt;- GET(\n  \"https://www.reed.co.uk/api/1.0/search?keywords=analyst&location=london&distancefromlocation=15\",\n  authenticate(\n    user = Sys.getenv(\"putyourapikeyhere\"),\n    password = \"\"\n  )\n)\n\n\n\n\n\n\n\nTip\n\n\n\nReplace Sys.getenv(\"putyourapikeyhere\") with your own API key.\n\n\n\nstatus_code(response)\n\n#&gt; [1] 200\n\n# Convert the JSON string to a dataframe and view data in a table\n\njob_dataframe &lt;- fromJSON(content(response, as = \"text\"), flatten = TRUE)\n\n# The job dataframe is inside the results\n\njob_dataframe$results %&gt;%\n  dim()\n\n#&gt; [1] 100  15\n\njob_dataframe$results %&gt;%\n  head()\n\n\n  \n\n\n\n\n\n\nimport requests\n\nimport pandas as pd\n\n# Set API endpoint and API key\n\nurl = \"https://www.reed.co.uk/api/1.0/search?keywords=analyst&location=london&distancefromlocation=15\"\n\napi_key = \"replace with your own API key\" \n\nBased on the instructions in the API documentation, you will need to include your API key for all requests in a basic authentication http header as the username, leaving the password empty.\n\n# Send a GET request to the API endpoint\n\nresponse = requests.get(url, auth = (api_key, ''))\n\n\n# Check if the request was successful\n\nprint(response.status_code)\n\n#&gt; 200\n\n# Parse the response JSON data\n\njob_json = response.json()\n\n# Convert to a pandas dataframe\n\n# The dataframe is inside the results\n\njob_dataframe = pd.json_normalize(job_json[\"results\"])\n\njob_dataframe.shape\n\n#&gt; (100, 15)\n\njob_dataframe.head()\n\n#&gt;       jobId  ...                                             jobUrl\n#&gt; 0  53334560  ...       https://www.reed.co.uk/jobs/analyst/53334560\n#&gt; 1  53119090  ...  https://www.reed.co.uk/jobs/finance-analyst/53...\n#&gt; 2  53262233  ...  https://www.reed.co.uk/jobs/dialler-analyst/53...\n#&gt; 3  52724870  ...  https://www.reed.co.uk/jobs/catastrophe-analys...\n#&gt; 4  53239157  ...  https://www.reed.co.uk/jobs/finance-analyst/53...\n#&gt; \n#&gt; [5 rows x 15 columns]\n\n\n\n\n\nYou can now use the data for your data science."
  },
  {
    "objectID": "blog/api-in-r-and-python/index.html#other-resources",
    "href": "blog/api-in-r-and-python/index.html#other-resources",
    "title": "How to fetch API data in R and Python",
    "section": "Other resources",
    "text": "Other resources\nYou can also watch Dean Chereden YouTube video on how to GET data from an API using R in RStudio.\n\n\nI hope you found this article informative. You can find its GitHub repository here. If you enjoyed reading this write-up, please follow me on Twitter and Linkedin for more updates on R, Python, and Excel for data science."
  },
  {
    "objectID": "blog/data-consolidation-csv/index.html#pre-requisites",
    "href": "blog/data-consolidation-csv/index.html#pre-requisites",
    "title": "Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nBefore proceeding, make sure you have the following libraries installed:\n\npip install pandas dask\n\nAdditionally, you can follow along by downloading the sample dataset here."
  },
  {
    "objectID": "blog/data-consolidation-csv/index.html#method-1-using-pandas",
    "href": "blog/data-consolidation-csv/index.html#method-1-using-pandas",
    "title": "Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python",
    "section": "Method 1: Using Pandas",
    "text": "Method 1: Using Pandas\nStep 1: Load all CSV files\nTo begin, we will use the glob module to list all files matching a specific pattern. This pattern will allow us to target multiple CSV files at once:\n\nfrom glob import glob\nimport pandas as pd\n\n# List all files matching the pattern\nfiles = sorted(glob('data/Sales_Data_*.csv'))\n\nfiles\n\n['data\\\\Sales_Data_01.csv',\n 'data\\\\Sales_Data_02.csv',\n 'data\\\\Sales_Data_03.csv',\n 'data\\\\Sales_Data_04.csv',\n 'data\\\\Sales_Data_05.csv',\n 'data\\\\Sales_Data_06.csv',\n 'data\\\\Sales_Data_07.csv',\n 'data\\\\Sales_Data_08.csv',\n 'data\\\\Sales_Data_09.csv',\n 'data\\\\Sales_Data_10.csv',\n 'data\\\\Sales_Data_11.csv',\n 'data\\\\Sales_Data_12.csv',\n 'data\\\\Sales_Data_13.csv',\n 'data\\\\Sales_Data_14.csv',\n 'data\\\\Sales_Data_15.csv',\n 'data\\\\Sales_Data_16.csv',\n 'data\\\\Sales_Data_17.csv',\n 'data\\\\Sales_Data_18.csv',\n 'data\\\\Sales_Data_19.csv',\n 'data\\\\Sales_Data_20.csv']\n\n\nHere, glob() finds all files with names matching the pattern Sales_Data_*.csv and sorts them for easier handling.\nStep 2: Load and merge CSV files into a single dataframe\nOnce we have the list of files, the next step is to read and concatenate them using Pandas:\n\n# Concatenate all CSV files into a single dataframe\n\nsales_pandas = pd.concat([pd.read_csv(f) for f in files], ignore_index = True)\n\nThis command reads each file in the list, creates a Pandas dataframe, and merges them into a single dataframe.\nStep 3: Verify the consolidated data\nFinally, inspect the data to ensure it was properly loaded:\n\nsales_pandas.head() # Display the first few rows of the dataframe\n\n\n\n\n\n\n\n\nRegion\nProduct\nDate\nSales\n\n\n\n\n0\nWest\nProd T\n9/6/2012\n53395.17732\n\n\n1\nWest\nProd K\n2/23/2016\n116609.69480\n\n\n2\nSouth\nProd F\n9/20/2013\n72524.09530\n\n\n3\nSouth\nProd J\n12/24/2010\n22538.47873\n\n\n4\nNorth\nProd D\n3/28/2012\n45616.53282\n\n\n\n\n\n\n\n\nsales_pandas.tail() # Display the last few rows of the dataframe\n\n\n\n\n\n\n\n\nRegion\nProduct\nDate\nSales\n\n\n\n\n1001\nWest\nProd L\n10/19/2010\n19017.514750\n\n\n1002\nNorth\nProd H\n6/17/2010\n13202.371870\n\n\n1003\nSouth\nProd K\n7/23/2015\n106101.309600\n\n\n1004\nSouth\nProd C\n3/31/2010\n9553.824099\n\n\n1005\nSouth\nProd I\n2/4/2014\n79516.456490\n\n\n\n\n\n\n\nThis step helps to confirm that the data has been successfully consolidated."
  },
  {
    "objectID": "blog/data-consolidation-csv/index.html#method-2-using-dask",
    "href": "blog/data-consolidation-csv/index.html#method-2-using-dask",
    "title": "Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python",
    "section": "Method 2: Using Dask",
    "text": "Method 2: Using Dask\nIf you are working with datasets that are too large to fit in memory, Dask is a great alternative to Pandas. Dask allows you to work with larger-than-memory datasets by breaking them into manageable chunks and processing them in parallel.\nStep 1: Install and import Dask\nFirst, ensure Dask is installed:\n\npip install dask\n\nThen, import Dask alongside Pandas:\n\nimport dask.dataframe as dd\n\nStep 2: Read and load files into a Dask dataframe\nLike Pandas, Dask can also load multiple CSV files. The difference is that Dask operates lazily, meaning it doesn’t load the data until it is necessary.\n\n# Read CSV files with Dask\n\nsales_dask = dd.read_csv('data/Sales_Data_*.csv')\n\nThis command will load all CSV files matching the pattern into a Dask dataframe.\nStep 3: Convert the Dask dataframe into a Pandas dataframe (if needed)\nOnce you’ve processed the data with Dask, you can compute the result and convert it to a Pandas dataframe, if needed:\n\nsales_pandas = sales_dask.compute()\n\nThis will trigger the computation and load the data into memory as a Pandas dataframe. Be cautious with this step, as the dataset needs to fit into memory for Pandas to handle it.\nStep 4: Analyze and verify the data\nAs with Pandas, you can inspect the Dask dataframe or the resulting Pandas dataframe:\n\nsales_pandas.head()\n\n\n\n\n\n\n\n\nRegion\nProduct\nDate\nSales\n\n\n\n\n0\nWest\nProd T\n9/6/2012\n53395.17732\n\n\n1\nWest\nProd K\n2/23/2016\n116609.69480\n\n\n2\nSouth\nProd F\n9/20/2013\n72524.09530\n\n\n3\nSouth\nProd J\n12/24/2010\n22538.47873\n\n\n4\nNorth\nProd D\n3/28/2012\n45616.53282\n\n\n\n\n\n\n\n\nsales_pandas.info(memory_usage = 'deep')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1006 entries, 0 to 45\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Region   1006 non-null   object \n 1   Product  1006 non-null   object \n 2   Date     1006 non-null   object \n 3   Sales    1006 non-null   float64\ndtypes: float64(1), object(3)\nmemory usage: 179.5 KB\n\n\nThis gives you a detailed look at the memory usage of your dataset, which is particularly helpful for large datasets."
  },
  {
    "objectID": "blog/export-r-dataframe-to-excel-workbook/index.html",
    "href": "blog/export-r-dataframe-to-excel-workbook/index.html",
    "title": "Export Multiple Data Frames to Different Excel Worksheets in R",
    "section": "",
    "text": "Introduction\nThe openxlsx2 package provides an efficient way to export multiple data frames from R into an Excel workbook, where each worksheet in the workbook corresponds to a specific data frame in R.\n\n\nPrerequisites\nThis tutorial begins by ensuring that the necessary packages are installed and loaded. If they are not already present in your R environment, the following code will download and install them. Note that an internet connection is required for this step.\n\nif (!require(install.load)) {\n  install.packages(\"install.load\")\n}\n\ninstall.load::install_load(c(\"openxlsx2\", \"readxl\", \"purrr\"))\n\n#&gt; package 'zip' successfully unpacked and MD5 sums checked\n#&gt; package 'openxlsx2' successfully unpacked and MD5 sums checked\n#&gt; \n#&gt; The downloaded binary packages are in\n#&gt;  C:\\Users\\admin\\AppData\\Local\\Temp\\Rtmpugp96y\\downloaded_packages\n\n\n\n\nPreparing the Data\nIn this example, we will export four commonly used datasets: airquality, mtcars, iris, and diamonds, each to a different worksheet within the same Excel workbook.\n\ndataframe1 &lt;- datasets::airquality\ndataframe1 |&gt; head()\n\n\n  \n\n\ndataframe2 &lt;- datasets::mtcars\ndataframe2 |&gt; head()\n\n\n  \n\n\ndataframe3 &lt;- datasets::iris\ndataframe3 |&gt; head()\n\n\n  \n\n\ndataframe4 &lt;- ggplot2::diamonds\ndataframe4 |&gt; head()\n\n\n  \n\n\n\n\n\nExporting Data Frames to Excel\nNow that we have our data frames prepared, we will assign them to individual worksheets in an Excel workbook. The worksheet names will correspond to the names of the data frames in R. For example:\n\nsheet1- airquality\nsheet2- mtcars\nsheet3- iris\nsheet4- diamonds\n\n\nlist_of_datasets &lt;- list(\n  \"airquality\" = dataframe1,\n  \"mtcars\" = dataframe2,\n  \"iris\" = dataframe3,\n  \"diamonds\" = dataframe4\n)\n\nwrite_xlsx(list_of_datasets, \"Excel_workbook.xlsx\")\n\nAfter running the code, you will find the Excel_workbook.xlsx file in your working directory.\n\n\nVerifying the Export\nYou can verify the content of the Excel workbook either by opening it in Excel or by using R. Below, we’ll check the worksheet names and inspect the contents of each worksheet using the readxl and purrr packages.\n\nexcel_sheets(\"Excel_workbook.xlsx\")\n\n#&gt; [1] \"airquality\" \"mtcars\"     \"iris\"       \"diamonds\"\n\n\nTo explore the content of each worksheet:\n\npath &lt;- \"Excel_workbook.xlsx\"\n\npath |&gt;\n  excel_sheets() |&gt;\n  set_names() |&gt;\n  map(read_excel, path = path)\n\n#&gt; $airquality\n#&gt; # A tibble: 153 × 6\n#&gt;    Ozone Solar.R  Wind  Temp Month   Day\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    41     190   7.4    67     5     1\n#&gt;  2    36     118   8      72     5     2\n#&gt;  3    12     149  12.6    74     5     3\n#&gt;  4    18     313  11.5    62     5     4\n#&gt;  5    NA      NA  14.3    56     5     5\n#&gt;  6    28      NA  14.9    66     5     6\n#&gt;  7    23     299   8.6    65     5     7\n#&gt;  8    19      99  13.8    59     5     8\n#&gt;  9     8      19  20.1    61     5     9\n#&gt; 10    NA     194   8.6    69     5    10\n#&gt; # ℹ 143 more rows\n#&gt; \n#&gt; $mtcars\n#&gt; # A tibble: 32 × 11\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#&gt;  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#&gt;  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#&gt;  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#&gt;  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#&gt;  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#&gt;  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#&gt;  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#&gt;  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#&gt; 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#&gt; # ℹ 22 more rows\n#&gt; \n#&gt; $iris\n#&gt; # A tibble: 150 × 5\n#&gt;    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt;           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n#&gt;  1          5.1         3.5          1.4         0.2 setosa \n#&gt;  2          4.9         3            1.4         0.2 setosa \n#&gt;  3          4.7         3.2          1.3         0.2 setosa \n#&gt;  4          4.6         3.1          1.5         0.2 setosa \n#&gt;  5          5           3.6          1.4         0.2 setosa \n#&gt;  6          5.4         3.9          1.7         0.4 setosa \n#&gt;  7          4.6         3.4          1.4         0.3 setosa \n#&gt;  8          5           3.4          1.5         0.2 setosa \n#&gt;  9          4.4         2.9          1.4         0.2 setosa \n#&gt; 10          4.9         3.1          1.5         0.1 setosa \n#&gt; # ℹ 140 more rows\n#&gt; \n#&gt; $diamonds\n#&gt; # A tibble: 53,940 × 10\n#&gt;    carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ 53,930 more rows\n\n\n\n\nConclusion\nThis tutorial demonstrated how to efficiently export multiple data frames to separate worksheets in an Excel workbook using R. The approach is highly customizable, allowing for easy integration into your data analysis workflow.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/scientists-should-have-a-website/index.html",
    "href": "blog/scientists-should-have-a-website/index.html",
    "title": "Every Scientist Should Have a Website",
    "section": "",
    "text": "Introduction\nIn today’s fast-paced academic landscape, establishing a strong online presence is crucial for scientists to showcase their research, connect with peers, and build their professional brand. Don’t get me wrong, a digital publication list is nice and cool; but having a personal website means much more than a mere collection of papers. It is a great tool to highlight our achievements, foster collaborations, and inspire others on a global scale. Let’s take a look at five compelling reasons why you should consider creating your own website as well.\n\n\nReason 1: Showcase Your Work\nImagine having a central hub where you can effectively present your research achievements, publications, and expertise to the scientific community and beyond. A personal website provides precisely that. By curating your work in one place, you can stand out in the competitive scientific landscape and make it easier for others to discover and appreciate your valuable contributions. This platform offers a great view of your research journey.\n\n\nReason 2: Expand Your Network\nCollaboration is key to scientific progress. Your personal website acts as a gateway, connecting you with fellow scientists, potential collaborators, and industry professionals. It opens doors to new opportunities and facilitates the expansion of your professional network. By providing contact information and writing about your research interests, you can attract like-minded individuals who share your passion and expertise. Through your website, you can foster collaborations, exchange ideas, and forge valuable connections.\n\n\nReason 3: Have Control Over Your Content\nIn today’s digital age, maintaining control over your online presence is essential. While social media platforms and other online channels offer great reach, they often come with limitations and constraints. A personal website grants you the freedom to share your research findings, insights, and perspectives on your own terms. You are not bound by character counts or algorithmic filters. This level of control allows you to maintain a professional online persona while freely expressing your thoughts and ideas.\n\n\nReason 4: Reach an Audience\nWhile traditional scientific publications are the currency of academia, their reach can be limited. A personal website breaks down these barriers and makes you reach a global audience. By sharing your research with the world, you can engage with science enthusiasts, inspire others, and make a lasting impact beyond academic papers. Whether your goal is to communicate complex scientific concepts to a broader audience or connect with fellow researchers on a global scale, a personal website will bring you one step closer to your goal.\n\n\nReason 5: Enhance Opportunities\nA personal website is a way for potential employers and academic institutions to find out about your work, achievements, and contributions. It offers them a comprehensive understanding of your expertise, research interests, and the impact of your work. By showcasing your accomplishments and highlighting your unique perspectives, you can enhance your chances of exciting career opportunities.\n\n\nHow to Start Today\nSo why wait? Take the leap and create your personal website today! To assist you, I have prepared a free tutorial and template to get you started on the right foot:\n\n\n\nConclusion\nHaving a personal website empowers you to showcase your work, expand your professional network, maintain control over your content, reach a global audience, and enhance your career opportunities. Remember: Your research is valuable, and you can create a personal website to put yourself out there and share your knowledge with others in no time.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\n\n\n Back to top"
  }
]