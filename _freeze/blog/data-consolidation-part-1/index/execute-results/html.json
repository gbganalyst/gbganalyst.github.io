{
  "hash": "0e8e49ea266f2181c1f2caeb73696a78",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python\"\ndescription: |\n  In this quick tutorial, you'll learn how to easily merge several CSV files into one using Python. Whether you're working with a few small files or large datasets, this guide shows you how to use Pandas for simple tasks or `Dask` when your data is too big to handle. It's a practical, straightforward approach for anyone looking to combine CSV files efficiently.\ndate: \"10-20-2022\"\njupyter: python3\ncategories: \n  - Python\nimage: pandas.png\nformat:\n  html:\n    df_print: paged\n    fig-cap-location: bottom\n    include-before-body: ../../html/margin_image.html\n    include-after-body: ../../html/blog_footer.html\n    code-fold: false\n    code-tools: true\neditor: \n  markdown: \n    wrap: sentence\nresources: \n  - \"pandas.png\"\n---\n\n\n# Introduction\n\nData consolidation is a crucial step in data preprocessing, particularly when you are working with multiple data sources or large datasets. This tutorial will walk you through the process of merging multiple CSV files into a single dataframe using Python. We will explore two methods: one using `Pandas` and another using `Dask`, which is designed to handle larger datasets that may not fit into your computer's memory.\n\n## Pre-requisites\n\nBefore proceeding, make sure you have the following libraries installed:\n\n::: {#f648c435 .cell execution_count=1}\n``` {.python .cell-code}\npip install pandas dask\n```\n:::\n\n\nAdditionally, you can follow along by downloading the sample dataset [here](https://drive.google.com/open?id=1sFL2MMELasLHEYtoxK1ian4dTxS7UV7S).\n\n# Method\n\n## Method 1: Using Pandas\n\n**Step 1: Load all CSV files**\n\nTo begin, we will use the `glob` module to list all files matching a specific pattern. This pattern will allow us to target multiple CSV files at once:\n\n::: {#fbca70aa .cell execution_count=2}\n``` {.python .cell-code}\nfrom glob import glob\nimport pandas as pd\n\n# List all files matching the pattern\nfiles = sorted(glob('data/Sales_Data_*.csv'))\n```\n:::\n\n\nHere, `glob()` finds all files with names matching the pattern `Sales_Data_*.csv` and sorts them for easier handling.\n\n**Step 2: Load and merge CSV files into a single dataframe**\n\nOnce we have the list of files, the next step is to read and concatenate them using Pandas:\n\n::: {#0c67d04f .cell execution_count=3}\n``` {.python .cell-code}\n# Concatenate all CSV files into a single dataframe\n\nsales_pandas = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n```\n:::\n\n\nThis command reads each file in the list, creates a Pandas dataframe, and merges them into a single dataframe.\n\n**Step 3: Verify the consolidated data**\n\nFinally, inspect the data to ensure it was properly loaded:\n\n::: {#b8a7d88f .cell execution_count=4}\n``` {.python .cell-code}\nsales_pandas.head() # Display the first few rows of the dataframe\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>Product</th>\n      <th>Date</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>West</td>\n      <td>Prod T</td>\n      <td>9/6/2012</td>\n      <td>53395.17732</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>West</td>\n      <td>Prod K</td>\n      <td>2/23/2016</td>\n      <td>116609.69480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>South</td>\n      <td>Prod F</td>\n      <td>9/20/2013</td>\n      <td>72524.09530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>South</td>\n      <td>Prod J</td>\n      <td>12/24/2010</td>\n      <td>22538.47873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>North</td>\n      <td>Prod D</td>\n      <td>3/28/2012</td>\n      <td>45616.53282</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3ec084ea .cell execution_count=5}\n``` {.python .cell-code}\nsales_pandas.tail() # Display the last few rows of the dataframe\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>Product</th>\n      <th>Date</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1001</th>\n      <td>West</td>\n      <td>Prod L</td>\n      <td>10/19/2010</td>\n      <td>19017.514750</td>\n    </tr>\n    <tr>\n      <th>1002</th>\n      <td>North</td>\n      <td>Prod H</td>\n      <td>6/17/2010</td>\n      <td>13202.371870</td>\n    </tr>\n    <tr>\n      <th>1003</th>\n      <td>South</td>\n      <td>Prod K</td>\n      <td>7/23/2015</td>\n      <td>106101.309600</td>\n    </tr>\n    <tr>\n      <th>1004</th>\n      <td>South</td>\n      <td>Prod C</td>\n      <td>3/31/2010</td>\n      <td>9553.824099</td>\n    </tr>\n    <tr>\n      <th>1005</th>\n      <td>South</td>\n      <td>Prod I</td>\n      <td>2/4/2014</td>\n      <td>79516.456490</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis step helps to confirm that the data has been successfully consolidated.\n\n\n## Method 2: Using Dask for Large Datasets\n\nIf you are working with datasets that are too large to fit in memory, `Dask` is a great alternative to Pandas. `Dask` allows you to work with larger-than-memory datasets by breaking them into manageable chunks and processing them in parallel.\n\n**Step 1: Install and import Dask**\n\nFirst, ensure `Dask` is installed:\n\n::: {#eac6d1e2 .cell execution_count=6}\n``` {.python .cell-code}\npip install dask\n```\n:::\n\n\nThen, import Dask alongside Pandas:\n\n::: {#9186d39f .cell execution_count=7}\n``` {.python .cell-code}\nimport dask.dataframe as dd\n```\n:::\n\n\n**Step 2: Read and load files into a Dask dataframe**\n\nLike Pandas, Dask can also load multiple CSV files. The difference is that Dask operates lazily, meaning it doesn’t load the data until it is necessary.\n\n::: {#bc657cf6 .cell execution_count=8}\n``` {.python .cell-code}\n# Read CSV files with Dask\n\nsales_dask = dd.read_csv('data/Sales_Data_*.csv')\n```\n:::\n\n\nThis command will load all CSV files matching the pattern into a Dask dataframe.\n\n**Step 3: Convert the Dask dataframe into a Pandas dataframe (if needed)**\n\nOnce you've processed the data with Dask, you can compute the result and convert it to a Pandas dataframe, if needed:\n\n::: {#b66991ed .cell execution_count=9}\n``` {.python .cell-code}\nsales_pandas = sales_dask.compute()\n```\n:::\n\n\nThis will trigger the computation and load the data into memory as a Pandas dataframe. Be cautious with this step, as the dataset needs to fit into memory for Pandas to handle it.\n\n**Step 4: Analyze and verify the data**\n\nAs with Pandas, you can inspect the Dask dataframe or the resulting Pandas dataframe:\n\n::: {#9d5ab328 .cell execution_count=10}\n``` {.python .cell-code}\nsales_pandas.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>Product</th>\n      <th>Date</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>West</td>\n      <td>Prod T</td>\n      <td>9/6/2012</td>\n      <td>53395.17732</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>West</td>\n      <td>Prod K</td>\n      <td>2/23/2016</td>\n      <td>116609.69480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>South</td>\n      <td>Prod F</td>\n      <td>9/20/2013</td>\n      <td>72524.09530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>South</td>\n      <td>Prod J</td>\n      <td>12/24/2010</td>\n      <td>22538.47873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>North</td>\n      <td>Prod D</td>\n      <td>3/28/2012</td>\n      <td>45616.53282</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#02a41ef5 .cell execution_count=11}\n``` {.python .cell-code}\nsales_pandas.info(memory_usage='deep')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 1006 entries, 0 to 45\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Region   1006 non-null   object \n 1   Product  1006 non-null   object \n 2   Date     1006 non-null   object \n 3   Sales    1006 non-null   float64\ndtypes: float64(1), object(3)\nmemory usage: 179.5 KB\n```\n:::\n:::\n\n\nThis gives you a detailed look at the memory usage of your dataset, which is particularly helpful for large datasets.\n\n# Conclusion\n\nWhen consolidating data from multiple CSV files, Pandas is a fantastic option for small- to medium-sized datasets. However, when handling larger datasets that exceed your computer’s memory, Dask provides an efficient alternative by allowing parallel processing.\n\nUnderstanding when to use Pandas or Dask depends largely on the size of the data you’re working with and your available system resources. Start with Pandas for small datasets, and switch to Dask as your data grows in complexity.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}