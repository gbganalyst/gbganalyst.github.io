{
  "hash": "e5e6cfa05330255d77c9d2093967e171",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Consolidation: How to Efficiently Merge Multiple CSV Files in Python\"\nformat:\n  docx:\n    toc: true\n    number-sections: true\n    highlight-style: github\njupyter: python3\n---\n\n\n\n# Introduction\n\nData consolidation is a crucial step in data preprocessing, particularly when you are working with multiple data sources or large datasets. This tutorial will walk you through the process of merging multiple CSV files into a single dataframe using Python. We will explore two methods: one using `Pandas` and another using `Dask`, which is designed to handle larger datasets that may not fit into your computer's memory.\n\n## Pre-requisites\n\nBefore proceeding, make sure you have the following libraries installed:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\npip install pandas dask\n```\n:::\n\n\nAdditionally, you can follow along by downloading the sample dataset [here](https://drive.google.com/open?id=1sFL2MMELasLHEYtoxK1ian4dTxS7UV7S).\n\n## Method 1: Using Pandas\n\n**Step 1: Load all CSV files**\n\nTo begin, we will use the `glob` module to list all files matching a specific pattern. This pattern will allow us to target multiple CSV files at once:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom glob import glob\nimport pandas as pd\n\n# List all files matching the pattern\nfiles = sorted(glob('Data/Sales_Data_*.csv'))\n```\n:::\n\n\nHere, `glob()` finds all files with names matching the pattern `Sales_Data_*.csv` and sorts them for easier handling.\n\n**Step 2: Load and merge CSV files into a single dataframe**\n\nOnce we have the list of files, the next step is to read and concatenate them using Pandas:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Concatenate all CSV files into a single dataframe\nsales_pandas = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n```\n:::\n\n\nThis command reads each file in the list, creates a Pandas dataframe, and merges them into a single dataframe.\n\n**Step 3: Verify the consolidated data**\n\nFinally, inspect the data to ensure it was properly loaded:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(sales_pandas.head())  # Display the first few rows of the dataframe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Region Product        Date         Sales\n0   West  Prod T    9/6/2012   53395.17732\n1   West  Prod K   2/23/2016  116609.69480\n2  South  Prod F   9/20/2013   72524.09530\n3  South  Prod J  12/24/2010   22538.47873\n4  North  Prod D   3/28/2012   45616.53282\n```\n:::\n:::\n\n\nThis step helps to confirm that the data has been successfully consolidated.\n\n\n\n## Method 2: Using Dask for Large Datasets\n\nIf you are working with datasets that are too large to fit in memory, Dask is a great alternative to Pandas. `Dask` allows you to work with larger-than-memory datasets by breaking them into manageable chunks and processing them in parallel.\n\n**Step 1: Install and import Dask**\n\nFirst, ensure `Dask` is installed:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npip install dask\n```\n:::\n\n\nThen, import Dask alongside Pandas:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport dask.dataframe as dd\n```\n:::\n\n\n**Step 2: Read and load files into a Dask dataframe**\n\nLike Pandas, Dask can also load multiple CSV files. The difference is that Dask operates lazily, meaning it doesn’t load the data until it is necessary.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Read CSV files with Dask\nsales_dask = dd.read_csv('Data/Sales_Data_*.csv')\n```\n:::\n\n\nThis command will load all CSV files matching the pattern into a Dask dataframe.\n\n**Step 3: Convert the Dask dataframe into a Pandas dataframe (if needed)**\n\nOnce you've processed the data with Dask, you can compute the result and convert it to a Pandas dataframe, if needed:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsales_pandas = sales_dask.compute()\n```\n:::\n\n\nThis will trigger the computation and load the data into memory as a Pandas dataframe. Be cautious with this step, as the dataset needs to fit into memory for Pandas to handle it.\n\n**Step 4: Analyze and verify the data**\n\nAs with Pandas, you can inspect the Dask dataframe or the resulting Pandas dataframe:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nprint(sales_pandas.info(memory_usage='deep'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 1006 entries, 0 to 45\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Region   1006 non-null   object \n 1   Product  1006 non-null   object \n 2   Date     1006 non-null   object \n 3   Sales    1006 non-null   float64\ndtypes: float64(1), object(3)\nmemory usage: 179.5 KB\nNone\n```\n:::\n:::\n\n\nThis gives you a detailed look at the memory usage of your dataset, which is particularly helpful for large datasets.\n\n# Conclusion\n\nWhen consolidating data from multiple CSV files, Pandas is a fantastic option for small- to medium-sized datasets. However, when handling larger datasets that exceed your computer’s memory, Dask provides an efficient alternative by allowing parallel processing.\n\nUnderstanding when to use Pandas or Dask depends largely on the size of the data you’re working with and your available system resources. Start with Pandas for small datasets, and switch to Dask as your data grows in complexity.\n\n---\n\nFor more detailed information on how to merge CSV files in Python, feel free to check out the [GitHub repository](https://github.com/gbganalyst/merge-csv-files-in-python). You can also follow me on [Twitter](https://www.twitter.com/gbganalyst) and [LinkedIn](https://www.linkedin.com/in/ezekiel-ogundepo/) for more updates on data science using Python, R, and Excel.\n\n",
    "supporting": [
      "index_files\\figure-pdf"
    ],
    "filters": []
  }
}